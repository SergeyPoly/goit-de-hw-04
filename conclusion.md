**Частина 1**

- Job 0: зчитування даних з файлу nuek-vuh3.csv.
- Job 1: розпізнавання типів (.option("inferSchema", "true")).
- Job 2: завантаження CSV і виконання базові трансформації.
- Job 3: підготока даних (shuffle) для агрегації або іншого групування.
- Job 4: виконання фінальної обробки і повернення даних у драйвер (collect()).

Висновок:\
У цій частині Spark проходить стандартний шлях: зчитування файлу → підготовка даних → їх групування → повернення результатів.

**Частина 2**

- Job 0: зчитування даних з файлу nuek-vuh3.csv.
- Job 1: розпізнавання типів (.option("inferSchema", "true")).
- Job 2: завантаження CSV і виконання базові трансформації.
- Job 3: підготока даних (shuffle) для агрегації або іншого групування.
- Job 4: виконання фінальної обробки і повернення даних у драйвер (collect()).
- Job 5: повторення аналогічних дій із Job 2.
- Job 6: повторення аналогічних дій із Job 3.
- Job 7: повторення аналогічних дій із Job 4.

Висновок:\
У цій частині після другого виклику collect() Spark повторно виконує всі ті ж самі обчислення.

**Частина 3**

- Job 0: зчитування даних з файлу nuek-vuh3.csv.
- Job 1: розпізнавання типів (.option("inferSchema", "true")).
- Job 2: завантаження CSV і виконання базові трансформації.
- Job 3: підготока даних (shuffle) для агрегації або іншого групування.
- Job 4: виконання фінальної обробки і повернення даних у драйвер (collect()).
- Job 5: повторення аналогічних дій із Job 3.
- Job 6: повторення аналогічних дій із Job 4.

Висновок:\
У цій частині знову відбувається повторне виконання обчислень,
але цього разу Job 2 не повторюється, тобто Spark, зберігає результати початкової трансформації (або оптимізує виконання).